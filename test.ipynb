{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from resources.data_loader import DataLoader\n",
    "from pipeline import get_default_config, plot_history\n",
    "from pipeline import classify_shapelets_mts, train_mts\n",
    "from pipeline import classify_shapelets_text, train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the MTS datasets\n",
    "mts_datasets = data_loader.get_mts_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Can put all parameters in the config dict\n",
    "#       Can then do whatever ablation studies / experiments with that\n",
    "config = get_default_config()\n",
    "config[\"stride\"] = 5\n",
    "for dataset in mts_datasets:\n",
    "    print(dataset)\n",
    "    X_train, y_train = data_loader.load_mts_dataset(dataset, split=\"train\")\n",
    "    X_test, y_test = data_loader.load_mts_dataset(dataset, split=\"test\")\n",
    "    # Filter for ragged datasets (e.g. JapaneseVowels)\n",
    "    if type(X_train) == list:\n",
    "        continue\n",
    "    history, encoder = train_mts(X_train, config, random_state=42, debug=False)\n",
    "    plot_history(history, f\"plots/encoder_training_{dataset}.pdf\")\n",
    "    \n",
    "    classify_shapelets_mts(X_train, y_train, X_test, y_test, config, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Text data loading\n",
    "X, y = data_loader.load_text_dataset(\"data\")\n",
    "train_indices, test_indices = train_test_split(np.arange(len(X)), stratify=y, train_size=20, test_size=100, random_state=42)\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_default_config()\n",
    "config['min_length'] = 5\n",
    "config['max_length'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history, encoder, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# plot and save history:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plot_history(history, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplots/encoder_training_text.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Korbi\\Documents\\Uni\\PhD\\courses\\Deep Learning\\Project\\code\\pipeline.py:162\u001b[0m, in \u001b[0;36mtrain_text\u001b[1;34m(X_train, config, random_state, debug)\u001b[0m\n\u001b[0;32m    160\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# No model call here, that is done in the loss function directly\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    164\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Korbi\\Documents\\Uni\\PhD\\courses\\Deep Learning\\Project\\code\\resources\\triplet_loss.py:179\u001b[0m, in \u001b[0;36mPNTripletLossText.forward\u001b[1;34m(self, batch, encoder)\u001b[0m\n\u001b[0;32m    176\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor([\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    178\u001b[0m e \u001b[38;5;241m=\u001b[39m EncoderBPE(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m--> 179\u001b[0m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m shapelets \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mget_filtered_shapelets(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Calculate similarities:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Korbi\\Documents\\Uni\\PhD\\courses\\Deep Learning\\Project\\code\\resources\\text.py:134\u001b[0m, in \u001b[0;36mEncoderBPE.fit\u001b[1;34m(self, texts, counts)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m best_n:\n\u001b[0;32m    133\u001b[0m         pairing, occurances, best_n \u001b[38;5;241m=\u001b[39m key, pairings[key], n\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pairing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# append new shapelet:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairings\u001b[38;5;241m.\u001b[39mappend(pairing)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history, encoder, tokenizer = train_text(X_train, config, random_state=42, debug=True)\n",
    "\n",
    "# plot and save history:\n",
    "plot_history(history, f\"plots/encoder_training_text.pdf\")\n",
    "\n",
    "# save encoder:\n",
    "torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "\n",
    "# save tokenizer:\n",
    "tokenizer.save('tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run shapelet classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<br /> <br />', 'I saw it, and I', 'News: I have to give', 'a lot of other so', \", but it this isn't\", 'in this film, the', 'new movie, which may', 'it one of the best', 'years, is like a', 'my review with the fact', 'she has is more then', 'this film is wonderful film for', 'etc.) But this movie', 'Zombie movie? \"I', 'draw the audience into the', 'thing with \"Freddy vs.', 'would not be out of', \"her husband's business,\", 'by the original when I was', 'knockout and getting to see', 'all that bad of a movie', ', as she was in her', 'a lot to like about this movie.', 'America. It helped me to', 'and you may find yourself']\n",
      "Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "classify_shapelets_text(X_train, y_train, X_test, y_test, config, tokenizer, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
